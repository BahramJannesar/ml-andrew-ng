<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته سوم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/</link>
    <description>Recent content in  هفته سوم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Sep 2020 18:30:44 +0430</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week3/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>طبقه بندی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/classification/</link>
      <pubDate>Thu, 10 Sep 2020 12:51:11 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/classification/</guid>
      <description>Logistic Regresstion اینجا از مسائل Regression به مسائل Classification می‌رویم، اما با اسم Logistic Regression گیج نشوید! این اسم به دلایل تاریخی نامگذاری شده که در واقع رویکردی برای حل مسائل طبقه بندی است نه رگرسیون!
Binary Classification به جای اینکه خروجی یعنی $y$ مقداری پیوسته در یک محدوده باشد، فقط $0$ یا $1$ است، یعنی : $ y \in \text{{0,1}} $
به طوری که معمولا به $0$، negative class و به $1$ هم positive class می‌گوییم، اما شما آزاد هستید که هر اسم دلخواهی را برای نام‌گذاری آن ها انتخاب کنید!</description>
    </item>
    
    <item>
      <title>تابع هزینه</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function/</link>
      <pubDate>Fri, 11 Sep 2020 11:12:45 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/cost-function/</guid>
      <description>ما نمی‌توانیم از همان تابعی هزینه ای که برای رگرسیون خطی استفاده کردیم، برای تابع لجستیک نیز استفاده کنیم، زیرا خروجی تابع لجستیک موج گونه است و باعث ایجاد تعداد زیادی مینیمم محلی می‌شود. به عبارت دیگر یک تابع محدب (convex) نیست.
تابع هزینه ما برای Logistic Regression به این صورت است:
$$ J(\theta) = \frac{1}{m} \sum_{i = 1}^m Cost(h_\theta(x^{(i)}, y^{(i)})) $$
$$ Cost(h_\theta(x), y) = -log(h_\theta(x)) \hspace{1cm} if \hspace{0.3cm} y = 1 $$</description>
    </item>
    
    <item>
      <title>ساده شده تابع هزینه و گرادیان کاهشی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/simplified-cost-gradient/</link>
      <pubDate>Fri, 11 Sep 2020 12:53:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/simplified-cost-gradient/</guid>
      <description>Cost Function ما می‌توانیم دو حالت شرطی تابع هزینه خودمان در قسمت قبلی را در یک حالت فشرده شده بنویسیم:
$$ Cost(h_\theta(x), y) = - y \hspace{0.2cm} log(h_\theta(x)) - (1 - y) log(1 - h_\theta(x)) $$
در خاطر داشته باشید وقتی که $y$ برابر $1$ است، قسمت $(1 - y) log(1 - h_\theta(x))$ برار $0$ خواهد شد.
اگر $y$ برابر با $1$ باشد، سپس قسمت $- y \hspace{0.2cm} log(h_\theta(x))$ برابر $0$ خواهد شد و در نتیجه تاثیری ندارد.</description>
    </item>
    
    <item>
      <title>بهینه سازی پیشرفته</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week3/advanced-optimization/</link>
      <pubDate>Fri, 11 Sep 2020 13:28:40 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week3/advanced-optimization/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>