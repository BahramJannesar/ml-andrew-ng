<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>دوره یادگیری ماشین دانشگاه استنفورد به فارسی on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/</link>
    <description>Recent content in دوره یادگیری ماشین دانشگاه استنفورد به فارسی on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Sep 2020 18:40:41 +0430</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>رگرسیون خطی چند متغیره</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/linear-regression-many-variable/</link>
      <pubDate>Wed, 09 Sep 2020 21:11:53 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/linear-regression-many-variable/</guid>
      <description>توی این هفته قراره در مورد رگرسیون خطی با چندین متغیر صحبت کنیم!
مثلا دیتا ای شبیه به این برای خانه ها را فرض کنید:
   نماد      $m$ تعداد کل سطر های جدول داده ها   $n$ تعداد ویژگی ها یا همان متغیر ها   $x^{(i)}$ i امین ردیف از جدول شامل متغیر ها   $x_j^{(i)}$ مقدار موجود در ردیف i ام و ستون متغیر j    بنابراین برای تابع فرضه داریم: $h_\theta = \theta_0 + \theta_1 + \theta_2x + &amp;hellip; + \theta_nx$</description>
    </item>
    
    <item>
      <title>یادگیری ماشین چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/what-is-ml/</link>
      <pubDate>Sat, 05 Sep 2020 18:40:41 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/what-is-ml/</guid>
      <description>دو تعریف از یادگیری ماشین ارائه شده است:
 Arthur Samuel: رشته مطالعاتی که به کامپیوتر ها این توانایی را می‌دهد که بدون برنامه نویسی صریح یاد بگیرند.
 توجه: این یک تعریف قدیمی و غیر رسمی است!
 اما تعریفی مدرن تر &amp;hellip;
 Tom Mitchell: به یک برنامه کامپیوتری گفته می‌شود که: برای یادگیری از تجربه E با توجه به برخی از وظایف به عنوان T و اندازه گیری عملکرد با P اگر عملکرد وظیفه T با استفاده از P اندازه گیری شود با استفاده از تجربه E بهبود یابد.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی چند متغیره</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/gradient-many-variable/</link>
      <pubDate>Wed, 09 Sep 2020 21:27:26 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/gradient-many-variable/</guid>
      <description>الگوریتم جدید ما برای گرادیان کاهشی با چندین متغیر به این صورت است:
و قسمت $ \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)} ) x_j^{(i)} $ همان مشتق جرئی $\frac {\partial} {\partial\theta_0} J(\theta)$ است.
به طور مثال برای دو متغیره و یا بیشتر خواهیم داشت:
یادآوری: مقدار $x_0$ برابر $1$ است.
 </description>
    </item>
    
    <item>
      <title>یادگیری نظارتی چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/supervised/</link>
      <pubDate>Sun, 06 Sep 2020 12:54:05 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/supervised/</guid>
      <description>تعریف یادگیری نظارتی در یـادگـیری نــظارتـی یک مجموعه داده داریم و از قبل می‌دانیم که خروجی صحیح باید چطور باشد، اصطلاحا داده های لیبل خورده اند! با این ایده که به بین خروجی و ورودی رابطه وجود دارد.
مسائل یادگیری نظارتی یا همان Supervised Learning به دو دسته رگرسیون و طبقه بندی تقسیم می‌شوند.
رگرسیون | Regression در این مسائل سعی می‌کنیم خروجی ای با مقدار پیوسته را پیش بینی کنیم.</description>
    </item>
    
    <item>
      <title>Feature Scaling</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/feature-scaling/</link>
      <pubDate>Wed, 09 Sep 2020 21:48:49 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/feature-scaling/</guid>
      <description>در این قسمت و قسمت بعدی در مورد فوت و فن هایی برای اعمال الگوریتم گرادیـــان کـــاهشی صحبت می‌کنیم.
اگر شما مسئله ای دارید که چندین ویژگی یا متغیر دارد و اگر مطمئن هستید که متغیر ها در مقیاس مشابه ای نسبت به هم هستند، در این حــالت گرادیــــان کـــاهشی با سرعت بیشتری به همگرایی می‌رسد.
فرض کنید مسئله ما دو متغیر به صورت زیر دارد: $$ x_1 = \text {size(0-2000 feet^2) }$$ $$ x_2 = \text {number of bedrooms(1-5) }$$</description>
    </item>
    
    <item>
      <title>یادگیری غیر نظارتی چیست؟</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/unsupervised/</link>
      <pubDate>Sun, 06 Sep 2020 12:56:04 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/unsupervised/</guid>
      <description>تعریف یادگیری غیر نظارتی یادگیری بدون نظارت این امکان را به ما مـی‌دهد کــه بدون داشتن هیچ ایده ای نسبت به خروجی داده ها به حل مشکلات نزدیک شویم. در واقع در اینجا داده های ما هیچ برچسبی نـدارنـد و الگوریتم‌ها به حال خود رها می‌شوند تا سـاختـارهــای موجود در میان داده‌ها را کشف کنند. Unsupervised Learning ها به دو دسته خوشه بندی و غیر خوشه بندی تقسیم می‌شوند.
خوشه بندی | Clustering در این مسـائـــل سـعــی مـی‌کــنیم داده هایی با ویژگی های مشترک را به چـندین گــروه تقـسیم کــنیم، یعنی آن ها را به خوشه ها تخصیص بدهیم.</description>
    </item>
    
    <item>
      <title>Debugging Gradient</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/debugging-gradient/</link>
      <pubDate>Wed, 09 Sep 2020 22:03:11 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/debugging-gradient/</guid>
      <description>در این قسمت در مورد تکنیک هایی برای اطمینان از درستی کار گرادیان کاهشی صحبت مـی‌کنیم. و در ادامه در مورد نحوه انتخاب مقدار پارامتر آلفا.
همانطور که می‌دانیم کار گرادیان کاهشی پیدا کردن مقدار تتا برای ما است تا تابع هزینه مینیمم شود. می‌خواهیم نمودار تابع $J$ بر حسب دفعات انــــجام گرادیان کاهشی را رسم کنیم و تا متوجه بشویم که گرادیان کاهشی عملکرد درستی دارد یا نه!
به این ترتیب نموداری به این شکل خواهیم داشت: می‌بینیم که احتملا گرادیان کاهشی درست کار مـی‌کند چون بعد از هر بار انجام مقدار $J$ کاهش می‌یابد!</description>
    </item>
    
    <item>
      <title>رگرسیون خطی با یک متغیر</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/linear-regression-one-variable/</link>
      <pubDate>Sun, 06 Sep 2020 13:26:16 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/linear-regression-one-variable/</guid>
      <description>بررسی نماد ها و مفاهیم مثلا در دیتای خانه ها نماد ها به این صورت هستند:    نماد      $m$ تعداد کل ردیف های جدول دیتای یادگیری   $x$ متغیر های ورودی   $y$ متغیر های خروجی یا هدف    برای آدرس دهی در جدول به این شکل عمل می‌کنیم:
$$(x_i, y_i) \Rightarrow x_1= 2104, y_1 = 460$$
اینجا منظور از $i$ اندیس دیتا در جدول است.</description>
    </item>
    
    <item>
      <title>رگرسیون چند جمله ای</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/polynomial-regression/</link>
      <pubDate>Wed, 09 Sep 2020 22:12:45 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/polynomial-regression/</guid>
      <description>Polynomial Regression تابع فرضیه $h$ می‌تواند خطی نباشد، اگر تناسب خوبی با داده های ما ندارد، می‌توانیم برای تغییر منحنی تابع از توابع چند جمله ای استفاده کنیم تا به تناسب بهتری برای داده ها برسیم.
فرض کنید که تابع فرضیه ما $ h_\theta(x) = \theta_0 + \theta_1 x_1$ باشد بنابراین می‌توانیم ویژگی جدیدی بر پایه ویژگی $x_1$ اضافه کنیم تا به تابعی quadratic یا درجه دوم برسیم:
$$ {\color{Blue} h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2}$$</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost1/</link>
      <pubDate>Sun, 06 Sep 2020 14:08:57 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost1/</guid>
      <description>تعریف تابع هزینه | Cost Function با این تابع می‌توانیم بهترین خط مستقیم را برای داده هایمان به دست آوریم. با انتخاب های متفاوت برای پارامتر های $\theta_1$ و $\theta_0$ تابع های فرضیه متفاوتی به دست می‌آوریم: در رگرسیون خطی مجموعه آموزشی مثل این نمودار داریم و می‌خواهیم مقادیری برای $\theta_0$ و $\theta_1$ به دست آوریم به طوری که خط راستی که رسم می‌کنیم، بهترین تطابق را با داده هایمان داشته باشد.</description>
    </item>
    
    <item>
      <title>معادله نرمال</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week2/normal-equation/</link>
      <pubDate>Thu, 10 Sep 2020 11:44:59 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week2/normal-equation/</guid>
      <description>Normal Equation الگوریتم گرادیان کاهشی روشی بود برای مینیمم کردن تابع $J$ ، اما روش دومی نیز وجود دارد که بدون داشتن حلقه تکرار این کار را انجام بدهد که معادله نرمال نام دارد.
فرض کنید که تابع هزینه درجه دو ای مثل این داریم: $$ J(\theta) = a\theta^2 + b\theta + c $$ $$ \frac{\partial} {\partial x} J(\theta) \overset{\underset{\mathrm{set}}{}}{=} 0 $$
که برای مینیمم کردن این تابع درجه دو مشتق آن را می‌گیریم و برابر با صفر قرار می‌دهیم، که این به ما اجازه می‌دهد که مقدار $\theta$ را برای مینیمم کردن تابع پیدا کنیم.</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost2/</link>
      <pubDate>Sun, 06 Sep 2020 14:26:42 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost2/</guid>
      <description>تا اینجا به طور خلاصه تمام چیزی که از تابع هزینه می‌دانیم در زیر آمده است:
اما اجازه بدید برای ساده سازی تابع فرضیه را تنها با یک پارامتر به این شکل در نظر بگیریم: $ h_\theta(x) = \theta_1x $ و سه مقدار مختلف $0$، $5.0 $ و $1$ رو حساب کنیم &amp;hellip;
مثلا برای مقدار تتا برابر با $1$ محاسبات زیر را خواهیم داشت:
$$ {\color{Red} J(\theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (\theta_1x - y_i)^2 \Rightarrow \frac{1}{2m} (0^2 + 0^2 + 0^2) = 0 } $$ به همین صورت برای دو مقدار دیگر داریم:</description>
    </item>
    
    <item>
      <title>تابع هزینه قسمت سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost3/</link>
      <pubDate>Sun, 06 Sep 2020 16:31:03 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/cost3/</guid>
      <description>قسمت قبل دیدیم که با داشتن فقط یک پارامتر برای تابع فرضیه، نمودار تابع هزینه یا همان $J$ به صورت سهمی بود. اگر دو پارامتر داشته باشیم باز هم به صورت سهمی است، اما سه بعدی و بسته به دیتای ما ممکن است به شکل زیر باشد:
اما ما برای نمایش این تابع از شکل سه بعدی استفاده نمی‌کنیم‌، بلکه از نمودار های کانتور استفاده می‌کنیم!
در این نمودار ها هر یک از بیضی ها نشان دهنده مجموعه ای از نقاط است که مقادیر یکسانی در $J$ بر حسب $\theta_0$ و $\theta_1$ های مختلف دارند.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient1/</link>
      <pubDate>Wed, 09 Sep 2020 16:31:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient1/</guid>
      <description>تعریف گرادیان کاهشی | Gradient Descent گرادیان کاهشی را برای مینیمم کردن تابع هزینه $J$ استفاده می‌کنیم. اما این الگوریتم تنها فقط در رگرسیون خطی کاربرد ندارد، بلکه در سایر قسمت های حوزه یادگیری ماشین نیز استفاده می‌شود.
مراحل کار به این شکل است:
با حدس های اولیه برای دو پارامتر $\theta_0$ و $\theta_1$ شروع می‌کنیم، مثلا مقدار هر دو را در ابتدا $0$ تعیین می‌کنیم.
و سپس مقادیر $\theta_0$ و $\theta_1$ را به صورت جزئی تغییر می‌دهیم تا تابع $J$ کاهش یابد، تا زمانی که به مینیمم کلی یا محلی برسیم.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient2/</link>
      <pubDate>Wed, 09 Sep 2020 16:31:43 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient2/</guid>
      <description>در قسمت قبل گرادیان کاهشی را به این صورت معرفی کردیم، در این قسمت می‌خواهیم به توضیح آلفا و عبارت مشتق بپردازیم. اما برای برای درک بهتر می‌خواهیم با یک مثال ساده تر تابعی با یک پارامتر را مینیمم کنیم، یعنی فرض می‌کنیم تابع هزینه $J$ فقط یک پارامتر دارد.
تصور کنید تابع $J$ زیر را با پارامتر تتا یک در این نقطه داریم، و از این نقطه کارمان را شروع می‌کنیم.</description>
    </item>
    
    <item>
      <title>گرادیان کاهشی قسمت سوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient3/</link>
      <pubDate>Wed, 09 Sep 2020 17:40:28 +0430</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week1/gradient3/</guid>
      <description>در این قسمت گرادیان کاهشی را با تابع هزینه ترکیب می‌کنیم و الگوریتم رگرسیون خطی را به دست می‌آوریم. تا اینجای کار به این ها رسیدیم:
اینجا می‌خواهیم از گرادیان کاهشی برای مینیمم کردن تابع هزینه استفاده کنیم! ابتدا تابع $J$ را در الگوریتم گرادیان جاگذاری می‌کنیم و &amp;hellip;
با محاسبه عبارت مشتق جزئی در گرادیان کاهشی برای دو پارامتر $\theta_0$ و $\theta_1$ خواهیم داشت:
$$ \theta_0, j = 0: \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) $$</description>
    </item>
    
  </channel>
</rss>